%!TEX root=./report.tex
\section{EVALUATION}

\subsection{Evaluation Metrics}
As a reminder, our task is identifying the correct object model from a large database, given a scan of a model in an uncluttered scene.

We evaluate the correctness of our results with the following metrics:
\begin{itemize}
  \item Percentage of ultimate guesses correct
  \item Confusion matrix for ultimate guess
  \item Precision-recall curve (using registration results)
  \item Average rank of correct model (using voting results)
  \item Area under the cumulative result-within-top-K histogram
\end{itemize}

The results for the Princeton Shape Benchmark can be seen in~\ref{tab:psb_results}
\begin{table}
  \centering
  \input{../results/PFH-FPFH-SHOT-SPIN_IMAGE_table.tex}
  \caption{Results on the Princeton Shape Benchmark.}
  \label{tab:psb_results}
\end{table}

An additional important evaluation is the runtime performance of different methods.
This data allows focused work on fast feature extraction and on algorithms to speed up matching, such as locality-sensitive hashing \cite{Frome2004}.
Hence, we report timing results, split by the different stages of our pipeline, for all experimental conditions.
In all cases, the tests were run on a 2.50 GHz Intel Core2 Quad Q9300 with 4 GB of RAM.

* table per dataset

* features vs. metrics

* plots of the same data that's in the tables?

* confusion matrix figures

* table of timing results

%\begin{figure}[thpb]
%   \centering
%   \label{fig:results}
%\end{figure}
