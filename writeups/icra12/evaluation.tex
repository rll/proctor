%!TEX root=./report.tex
\section{EVALUATION}

As a reminder, our task is identifying the correct object model from a large database, given a scan of a model in an uncluttered scene.
The detector should output one answer to this problem, although we also evaluate the process by which it arrives at the final answer.
In our evaluations, we care about both accuracy and speed of the detector.

\subsection{Accuracy Dashboard}

We evaluate the correctness of our results via the following scalar values
\begin{itemize}
  \item Percentage of final results that are correct. This gives a rough idea of how well the detector is performing.
  \item The Average Precision (AP), which is the area under the Precision-Recall curve for the registration accuracy of the final result. The AP is one of the most common scalar metrics of performance in computer vision~\cite{pascal-voc-2010}.
  \item Average Rank (AR) of correct model (using preliminary voting results). This number speaks to how well the features can get the right answer without any registration steps.
  \item Area Under the Cumulative Result-within-top-K histogram (AUCR) (using preliminary voting results). This gives a complementary picture to the average rank, as it describes the shape of the histogram instead of merely its mean. The maximum value is $1$, and the minimum value is $0.5$.
\end{itemize}

\begin{figure*}[thpb]
\centering
  \subfloat[Table of scalar measures of performance. The best number in each row is emphasized.]{
\label{tab:PSB_results}
\input{../results/PSB_PFH-FPFH-SHOT-SPIN_IMAGE_table.tex}
  } \\
  \subfloat[Precision-Recall curves, generated by sorting trials by the final registration distance to the ground truth.]{
\label{fig:PSB_pr}\includegraphics[width=0.45\linewidth]{../figures/PSB/PFH-FPFH-SHOT-SPIN_IMAGE_pr.png}
} \hfill \subfloat[Cumulative rank histogram shows the proportion of trials in which the correct result was fetched by the features in the initial stage.]{
\label{fig:PSB_rankhist}\includegraphics[width=0.45\linewidth]{../figures/PSB/PFH-FPFH-SHOT-SPIN_IMAGE_rankhist.png}
} \\
  \subfloat[PFH]{\includegraphics[width=0.45\linewidth]{../figures/PSB/PFH_confmat.png}} \hfill
  \subfloat[FPFH]{\includegraphics[width=0.45\linewidth]{../figures/PSB/FPFH_confmat.png}}\\
  \subfloat[SHOT]{\includegraphics[width=0.45\linewidth]{../figures/PSB/SHOT_confmat.png}} \hfill
  \subfloat[Spin Image]{\includegraphics[width=0.45\linewidth]{../figures/PSB/SPIN_IMAGE_confmat.png}}
  \caption{Dashboard for the accuracy-related performance of the different features on the Princeton Shape Benchmark dataset.}
  \label{fig:PSB_dashboard}
\end{figure*}

These scalar metrics, in table form, form one part of our Accuracy Dashboard, seen in~\autoref{fig:PSB_dashboard} for the Princeton Shape Benchmark.
The idea of the Dashboard is that each run of Proctor generates this complete set of results, all available in one place.
Proctor outputs all figures and data for the dashboard, and generates an HTML page or a PDF document tying them together.
In fact, ~\autoref{fig:PSB_dashboard} is an example of such an automatically-generated document---we simply inserted it into this paper's source.

In addition to the scalar metrics, which are useful for a summary comparison of different features or algorithms, the Dashboard provides visualization of three things:
\begin{itemize}
  \item For each feature, the confusion matrix between final results. This can be a useful tool to understand the behavior of the detector and a source of information for tuning the approach.
  \item Precision-Recall curves comparing all features. It is valuable to know whether a feature results in High-Precision, Low-Recall performance (such as requested by the Solutions in Perception Challenge \cite{SIPC2011}) or Low-Precision, High-Recall performance. By itself, the AP does not provide this information.
  \item The cumulative result-within-top-K histogram comparing all features. Once again, although we have both the Average Rank and the AUCR measures, it is valuable to see the shape of the histogram.
\end{itemize}

\subsection{Timing Dashboard}

Correctness is but one side of performance.
Timing information is crucial when selecting which feature to use for a task: a real-time robotics application has fundamentally different constraints than an offline large database matching application.

\begin{figure*}[thpb]
\centering
\subfloat[Total time spent in training and testing phases of the pipeline.]{\includegraphics[width=0.95\linewidth]{../figures/PSB/PFH-FPFH-SHOT-SPIN_IMAGE_timing_overview.png}} \\
\vspace{1em}
\subfloat[Breakdown of where time is spent in the testing phase.]{\includegraphics[width=0.95\linewidth]{../figures/PSB/PFH-FPFH-SHOT-SPIN_IMAGE_timing_test_panel.png}} 
\caption{Timing results on the Princeton Shape Benchmark dataset.}
\label{fig:PSB_timing}
\end{figure*}

The Timing Dashboard, seen in~\autoref{fig:PSB_timing} for the Princeton Shape Benchmark, provides both overall and detailed information about where time is spent in the pipeline.
This data can allow focused work on fast feature extraction and on algorithms to speed up matching, such as locality-sensitive hashing \cite{Frome2004}.

The top-level view of timing is provided by parts (a) and (b) of the Dashboard

In this paper, in all cases, the tests were run on a 2.50 GHz Intel Core2 Quad Q9300 with 4 GB of RAM.

\begin{figure*}[thpb]
\centering
\subfloat[[Total time spent in training and testing phases of the pipeline.]{\includegraphics[width=0.95\linewidth]{../figures/WGDB/PFH-FPFH-SHOT-SPIN_IMAGE_timing_overview.png}}
\vspace{1em}
\subfloat[Breakdown of where time is spent in the testing phase.]{\includegraphics[width=0.95\linewidth]{../figures/WGDB/PFH-FPFH-SHOT-SPIN_IMAGE_timing_test_panel.png}} 
\caption{Timing results on the Willow Garage Grasping dataset.}
\label{fig:WGDB_timing}
\end{figure*}

\begin{figure*}[thpb]
\centering
  \subfloat[Table of scalar measures of performance. The best number in each row is emphasized.]{
\label{tab:WGDB_results}
\input{../results/WGDB_PFH-FPFH-SHOT-SPIN_IMAGE_table.tex}
  } \\
  \subfloat[Precision-Recall curves, generated by sorting trials by the final registration distance to the ground truth.]{
\label{fig:WGDB_pr}\includegraphics[width=0.45\linewidth]{../figures/WGDB/PFH-FPFH-SHOT-SPIN_IMAGE_pr.png}
} \hfill \subfloat[Cumulative rank histogram shows the proportion of trials in which the correct result was fetched by the features in the initial stage.]{
\label{fig:WGDB_rankhist}\includegraphics[width=0.45\linewidth]{../figures/WGDB/PFH-FPFH-SHOT-SPIN_IMAGE_rankhist.png}
} \\
  \subfloat[PFH]{\includegraphics[width=0.45\linewidth]{../figures/WGDB/PFH_confmat.png}} \hfill 
  \subfloat[FPFH]{\includegraphics[width=0.45\linewidth]{../figures/WGDB/FPFH_confmat.png}}\\
  \subfloat[SHOT]{\includegraphics[width=0.45\linewidth]{../figures/WGDB/SHOT_confmat.png}} \hfill
  \subfloat[Spin Image]{\includegraphics[width=0.45\linewidth]{../figures/WGDB/SPIN_IMAGE_confmat.png}}
  \caption{Dashboard for the accuracy-related performance of the different features on the Willow Garage Grasping dataset.}
  \label{fig:WGDB_dashboard}
\end{figure*}
